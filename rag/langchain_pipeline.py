from deep_translator import GoogleTranslator
from langdetect import detect  # ‚úÖ Language detector
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# üß† LLM + Embeddings
embedding_model = OllamaEmbeddings(model="mistral")
llm_model = Ollama(model="mistral")

# Vector DB
vectorstore = Chroma(
    persist_directory="embeddings/chroma",
    embedding_function=embedding_model
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
qa_chain = RetrievalQA.from_chain_type(llm=llm_model, retriever=retriever)

# üåê Translate
def translate(text: str, source: str, target: str) -> str:
    try:
        return GoogleTranslator(source=source, target=target).translate(text)
    except:
        return text

# üéØ Main function
def answer_query(query: str) -> str:
    # Step 1: Detect user language
    lang = detect(query)
    if lang not in ['en', 'bn']:
        lang = 'en'  # fallback if unsure

    # Step 2: Translate query to English
    query_en = translate(query, source='auto', target='en')

    # Step 3: Run through RAG
    result = qa_chain.invoke({"query": query_en})
    answer_en = result.get("result", "").strip()

    if not answer_en:
        return "‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶™‡¶æ‡¶á‡¶®‡¶ø‡•§" if lang == "bn" else "Sorry, I couldn't find the answer."

    # Step 4: Translate back to original language
    if lang == "bn":
        return translate(answer_en, source='en', target='bn')
    else:
        return answer_en
    # Add at the bottom of langchain_pipeline.py for testing
if __name__ == "__main__":
    query = "What is Napa?"
    docs = vectorstore.similarity_search(query, k=5)
    for i, doc in enumerate(docs):
        print(f"Result {i+1}:\n{doc.page_content}\n{'-'*40}")
