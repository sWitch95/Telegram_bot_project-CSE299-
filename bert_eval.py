# bert_eval.py
import json
import torch
from bert_score import score
from rag.langchain_pipeline import answer_query

TEST_FILE = "data/test_hundred_queries.json"

def main():
    with open(TEST_FILE, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    candidate_answers = []
    reference_answers = []

    for i, item in enumerate(test_data, 1):
        query = item["query"]
        reference = item["reference"]

        # Get bot's answer
        bot_answer = answer_query(query)

        candidate_answers.append(bot_answer)
        reference_answers.append(reference)

        print(f"[{i}] Query: {query}")
        print(f"Bot Answer: {bot_answer[:80]}...")
        print(f"Reference: {reference[:80]}...")
        print("-" * 60)

    # Compute BERTScore
    P, R, F1 = score(candidate_answers, reference_answers, lang="en", verbose=True)

    print(f"\nğŸ“Š Average Precision: {P.mean().item():.4f}")
    print(f"ğŸ“Š Average Recall:    {R.mean().item():.4f}")
    print(f"ğŸ“Š Average F1 Score:  {F1.mean().item():.4f}")

if __name__ == "__main__":
    # GPU à¦¹à¦²à§‡ à¦à¦Ÿà¦¾ use à¦¹à¦¬à§‡
    if torch.cuda.is_available():
        print("âš¡ Using GPU for BERTScore")
    else:
        print("ğŸ¢ Using CPU for BERTScore")
    main()
